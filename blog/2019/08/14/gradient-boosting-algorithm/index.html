
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Gradient Boosting 算法的一些资料 - Steven's Blog</title>
  <meta name="author" content="Steven Sun">

  
  <meta name="description" content="想要了解xgboost，至少需要了解以下的一些概念：
梯度, Boosting, 分类器, 决策树, 概率分布, CART, 损失函数, 分裂准则, 加法模型, 叶子节点, 分裂点, 学习率, 分类, 回归, 初始化, 泰勒公式, 贪心法, 信息增益, 信息增益比, 特征, 特征值, 直方图算法 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://isunix.github.io/blog/2019/08/14/gradient-boosting-algorithm">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Steven's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  

</head>

<body    class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Steven's Blog</a></h1>
  
    <h2>A Dream Land of Peace!</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:isunix.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about" class="nav-link">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Gradient Boosting 算法的一些资料</h1>
    
    
      <p class="meta">
        








  


<time datetime="2019-08-14T13:51:36+08:00" pubdate data-updated="true">2019 年8 月14 日</time>
        
      </p>
    
  </header>


<div class="entry-content"><h2 id="xgboost">想要了解xgboost，至少需要了解以下的一些概念：</h2>
<p><code>梯度</code>, <code>Boosting</code>, <code>分类器</code>, <code>决策树</code>, <code>概率分布</code>, <code>CART</code>, <code>损失函数</code>, <code>分裂准则</code>, <code>加法模型</code>, <code>叶子节点</code>, <code>分裂点</code>, <code>学习率</code>, <code>分类</code>, <code>回归</code>, <code>初始化</code>, <code>泰勒公式</code>, <code>贪心法</code>, <code>信息增益</code>, <code>信息增益比</code>, <code>特征</code>, <code>特征值</code>, <code>直方图算法</code>, <code>DenseVector</code>, <code>凸函数</code>, <code>弱学习器</code>, <code>强学习器</code></p>

<h2 id="gradient-boosting-algorithm-">Gradient Boosting Algorithm 算法参考链接：</h2>

<ul>
  <li><a href="https://arxiv.org/pdf/1603.02754v1.pdf">XGBoost: A Scalable Tree Boosting System</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/">Which algorithm takes the crown: Light GBM vs XGBOOST?</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/">Learn Gradient Boosting Algorithm for better predictions (with codes in R)</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/">Getting smart with Machine Learning – AdaBoost and Gradient Boost</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/">Quick Introduction to Boosting Algorithms in Machine Learning</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">Complete Guide to Parameter Tuning in XGBoost with codes in Python</a></li>
  <li><a href="https://segmentfault.com/a/1190000014040317">XGboost数据比赛实战之调参篇</a></li>
  <li><a href="http://www.sohu.com/a/226265476_609569">通俗的将Xgboost的原理讲明白</a></li>
  <li><a href="https://blog.csdn.net/app_12062011/article/details/52136117">决策树</a> 和 <a href="https://blog.csdn.net/zhaocj/article/details/50503450">决策树源码剖析</a></li>
  <li><a href="[http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/](http://matafight.github.io/2017/03/14/XGBoost-简介/)">XGBoost 笔记</a></li>
  <li><a href="https://blog.csdn.net/liuzonghao88/article/details/88808408">XGBoost如何避免过拟合</a></li>
  <li><a href="https://blog.csdn.net/u012735708/article/details/83651832">XGBoost调参笔记</a></li>
  <li><a href="https://blog.csdn.net/a358463121/article/details/68617389">xgboost中的数学原理</a></li>
  <li><a href="https://blog.csdn.net/sb19931201/article/details/52557382">xgboost入门与实战</a></li>
  <li><a href="https://blog.csdn.net/xiaocong1990/article/details/55107239">XGBoost参数调优</a></li>
  <li><a href="https://blog.csdn.net/Totoro1745/article/details/53328725">xgboost：一个纯小白的学习历程</a></li>
  <li><a href="https://blog.csdn.net/github_38414650/article/details/76061893">通俗、有逻辑的写一篇说下Xgboost的原理</a></li>
  <li><a href="https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3">Boosting algorithm: GBM</a></li>
  <li><a href="https://www.imooc.com/article/43784?block_id=tuijian_wz">LightGBM 调参方法</a></li>
  <li><a href="https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/89909">Good summary of XGBoost vs CatBoost vs LightGBM</a></li>
  <li><a href="https://www.kaggle.com/samratp/lightgbm-xgboost-catboost">LightGBM + XGBoost + Catboost</a></li>
  <li><a href="https://datascience.stackexchange.com/questions/49567/lightgbm-vs-xgboost-vs-catboost">lightgbm-vs-xgboost-vs-catboost</a></li>
  <li><a href="https://medium.com/kaggle-nyc/gradient-boosting-decision-trees-xgboost-vs-lightgbm-and-catboost-72df6979e0bb">Gradient Boosting Decision trees: XGBoost vs LightGBM (and catboost)</a></li>
  <li><a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db">CatBoost vs. Light GBM vs. XGBoost</a></li>
  <li><a href="https://blog.csdn.net/LrS62520kV/article/details/79620615">CatBoost vs. Light GBM vs. XGBoost 的中文翻译</a></li>
  <li><a href="https://stackoverflow.com/questions/44937698/lightgbm-oserror-library-not-loaded">关于lightgbm的安装</a></li>
  <li><a href="https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d">Custom Loss Functions for Gradient Boosting</a></li>
  <li><a href="https://github.com/Microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions">machine-learning-challenge-winning-solutions-lightgbm-winned</a></li>
  <li><a href="https://www.jianshu.com/p/b4ac0596e5ef">LightGBM 如何调参</a></li>
  <li><a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/6665239.html">xgboost调参</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/57601606">为什么负梯度是函数值减小的最快方向</a></li>
  <li><a href="https://blog.csdn.net/qq_22238533/article/details/79185969">GBDT原理与Sklearn源码分析-回归篇</a></li>
  <li><a href="https://blog.csdn.net/qq_22238533/article/details/79192579?utm_medium=distribute.pc_relevant.none-task-blog-title-6&amp;spm=1001.2101.3001.4242">GBDT原理与Sklearn源码分析-分类篇</a></li>
  <li><a href="https://blog.csdn.net/qq_22238533/article/details/79199605">GBDT原理与实践-多分类篇</a></li>
  <li><a href="https://cloud.tencent.com/developer/article/1513111">XGBoost超详细推导，终于有人讲明白了</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[珍藏版</td>
          <td>20道XGBoost面试题](https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&amp;mid=2247485159&amp;idx=1&amp;sn=d429aac8370ca5127e1e786995d4e8ec&amp;chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&amp;scene=21#wechat_redirect)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting wiki Page</a></li>
  <li><a href="http://link.zhihu.com/?target=http%3A//www.saedsayad.com/docs/xgboost.pdf">xgboost introduction by xgboost-R’s author</a></li>
  <li><a href="https://blog.csdn.net/zpalyq110/article/details/79527653">GBDT算法原理以及实例理解 - 非常好
</a></li>
</ul>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Steven Sun</span></span>

      








  


<time datetime="2019-08-14T13:51:36+08:00" pubdate data-updated="true">2019 年8 月14 日</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/data-and-ml-and-ai/'>Data&ML&AI</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2019/08/01/shi-yong-sparkde-mllibjin-xing-ji-qi-xue-xi/" title="Previous Post: 使用Spark的MLlib进行机器学习">&laquo; 使用Spark的MLlib进行机器学习</a>
      
      
        <a class="basic-alignment right" href="/blog/2019/08/27/sparkzhong-pai-xu-shu-chu/" title="Next Post: Spark中排序输出">Spark中排序输出 &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2024/08/29/2022nian-de-gong-zuo-he-sheng-huo-jing-li/">2022年的工作和生活经历</a>
      </li>
    
      <li class="post">
        <a href="/blog/2021/01/04/jupytershang-shi-yong-matplotlibhua-tu-de-%5B%3F%5D-xie-you-yong-de-she-zhi/">Jupyter上使用matplotlib画图的一些有用的设置</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/12/11/matplotlibhe-seabornshi-yong-fang-fa-ji-jin/">Matplotlib和Seaborn使用方法集锦</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/12/11/pandasshi-yong-tipsji-jin/">Pandas使用tips集锦</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/11/20/sparksqlru-men-he-jin-jie/">SparkSQL入门和进阶</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/02/16/gao-deng-shu-li-tong-ji-xue-qian-yan-zhai-lu/">高等数理统计学前言摘录</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/27/%5B%3F%5D-xie-guan-yu-rnnhe-lstmhe-grude-yue-du-cai-liao/">一些关于RNN和LSTM和GRU的阅读材料</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/27/ru-he-cha-kan-linuxzhong-de-nei-cun-zhan-yong-qing-kuang/">如何查看Linux中的内存占用情况</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/27/sparkzhong-pai-xu-shu-chu/">Spark中排序输出</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/14/gradient-boosting-algorithm/">Gradient Boosting 算法的一些资料</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Categories</h1>
  <ul id="categories">
    <li class='category'><a href='/blog/categories/algorithms/'>Algorithms (8)</a></li>
<li class='category'><a href='/blog/categories/bigdata/'>BigData (10)</a></li>
<li class='category'><a href='/blog/categories/blog/'>Blog (7)</a></li>
<li class='category'><a href='/blog/categories/c/'>C (3)</a></li>
<li class='category'><a href='/blog/categories/data-and-ml-and-ai/'>Data&ML&AI (13)</a></li>
<li class='category'><a href='/blog/categories/django-and-flask/'>Django&Flask (4)</a></li>
<li class='category'><a href='/blog/categories/editor/'>Editor (2)</a></li>
<li class='category'><a href='/blog/categories/javascript/'>Javascript (4)</a></li>
<li class='category'><a href='/blog/categories/linux-and-mac/'>Linux&Mac (13)</a></li>
<li class='category'><a href='/blog/categories/php/'>PHP (5)</a></li>
<li class='category'><a href='/blog/categories/perl/'>Perl (47)</a></li>
<li class='category'><a href='/blog/categories/python/'>Python (11)</a></li>
<li class='category'><a href='/blog/categories/ruby/'>Ruby (1)</a></li>
<li class='category'><a href='/blog/categories/scala/'>Scala (1)</a></li>
<li class='category'><a href='/blog/categories/shell/'>Shell (12)</a></li>
<li class='category'><a href='/blog/categories/gan-wu/'>感悟 (1)</a></li>

  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2024 - Steven Sun -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
